{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure:\n",
    "\n",
    "1. Imports\n",
    "\n",
    "2. Create Fully Connected Network\n",
    "\n",
    "3. Set Device\n",
    "\n",
    "4. Hyperparameters\n",
    "\n",
    "5. Load Data (Simple MNIST)\n",
    "\n",
    "6. Initialize network\n",
    "\n",
    "7. Loss and Optimizer\n",
    "\n",
    "8. Train Network\n",
    "\n",
    "9. Check accuracy on training and test to see how good is our model (Eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn # all neural network modules, nn.Linear, nn.Conv2d, BatchNorm, loss functions\n",
    "import torch.optim as optim # all optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # all functions that dont have any parameters eg: activations like relu\n",
    "from torch.utils.data import DataLoader # gives easier dataset management and creates mini batches\n",
    "import torchvision.datasets as datasets # standard public datasets \n",
    "import torchvision.transforms as transforms # transforms on dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Fully Connected Network\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_size, \n",
    "                             out_features=50) #hidden layer = 50 nodes\n",
    "        self.fc2 = nn.Linear(in_features=self.fc1.out_features,\n",
    "                             out_features=num_classes)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Hyperparameters\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size=64\n",
    "num_epochs=10\n",
    "n_iterations = math.ceil(len(train_dataset)/batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Load Data (Simple MNIST)\n",
    "train_dataset=datasets.MNIST(root='dataset/', train=True, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_dataset=datasets.MNIST(root='dataset/', train=False, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                          shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 6. Initialize network\n",
    "model = FCN(input_size=input_size, num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7. Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Batch_size/Iterations: 0/938, Train Loss: 0.04015912115573883\n",
      "Epoch : 0 Batch_size/Iterations: 300/938, Train Loss: 0.051120657473802567\n",
      "Epoch : 0 Batch_size/Iterations: 600/938, Train Loss: 0.04207490384578705\n",
      "Epoch : 0 Batch_size/Iterations: 900/938, Train Loss: 0.16693073511123657\n",
      "Epoch : 2 Batch_size/Iterations: 0/938, Train Loss: 0.04357149451971054\n",
      "Epoch : 2 Batch_size/Iterations: 300/938, Train Loss: 0.059984687715768814\n",
      "Epoch : 2 Batch_size/Iterations: 600/938, Train Loss: 0.018003828823566437\n",
      "Epoch : 2 Batch_size/Iterations: 900/938, Train Loss: 0.03837967664003372\n",
      "Epoch : 4 Batch_size/Iterations: 0/938, Train Loss: 0.012316723354160786\n",
      "Epoch : 4 Batch_size/Iterations: 300/938, Train Loss: 0.006459308788180351\n",
      "Epoch : 4 Batch_size/Iterations: 600/938, Train Loss: 0.0842670127749443\n",
      "Epoch : 4 Batch_size/Iterations: 900/938, Train Loss: 0.03239819407463074\n",
      "Epoch : 6 Batch_size/Iterations: 0/938, Train Loss: 0.008761021308600903\n",
      "Epoch : 6 Batch_size/Iterations: 300/938, Train Loss: 0.03827043995261192\n",
      "Epoch : 6 Batch_size/Iterations: 600/938, Train Loss: 0.05899495258927345\n",
      "Epoch : 6 Batch_size/Iterations: 900/938, Train Loss: 0.028824325650930405\n",
      "Epoch : 8 Batch_size/Iterations: 0/938, Train Loss: 0.0067792958579957485\n",
      "Epoch : 8 Batch_size/Iterations: 300/938, Train Loss: 0.007825051434338093\n",
      "Epoch : 8 Batch_size/Iterations: 600/938, Train Loss: 0.03524378314614296\n",
      "Epoch : 8 Batch_size/Iterations: 900/938, Train Loss: 0.059359073638916016\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Train Network\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device) # torch.Size([64, 1, 28, 28]), torch.Size([64])\n",
    "        data=data.view(data.shape[0], -1) # remove the one channel NxHxW torch.Size([64, 784])\n",
    "\n",
    "        # clean past gradients collected during backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass - compute predictions\n",
    "        logits = model(data)\n",
    "        train_loss = criterion(logits, targets)\n",
    "        \n",
    "        if epoch % 2 == 0 and batch_idx % 300 == 0:\n",
    "            print(f\"Epoch : {epoch} Batch_size/Iterations: {batch_idx}/{n_iterations}, Train Loss: {train_loss}\")\n",
    "\n",
    "        # Backward Pass - get the gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update our weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on train data\n",
      "Accuracy achieved : 99.48 on dataset: 60032 and mean loss : 0.020\n",
      "Checking accuracy on test data\n",
      "Accuracy achieved : 97.18 on dataset: 10048 and mean loss : 0.109\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 9. Check accuracy on training and test to see how good is our model (Eval)\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on train data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "    num_correct=num_samples=0\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            data=data.view(data.shape[0], -1)\n",
    "\n",
    "            logits=model(data) # (batch_size, num_classes)\n",
    "            \n",
    "            # we need max index in dim=1 which holds num_classes values\n",
    "            prediction_index = torch.argmax(torch.softmax(logits, dim=1), dim=1) # torch.size([64])\n",
    "            num_correct += (prediction_index == targets).sum()\n",
    "            num_samples += prediction_index.shape[0]\n",
    "\n",
    "            val_loss.append(criterion(logits, targets).item()) # add losses for each batch\n",
    "    \n",
    "    acc = (num_correct / num_samples) * 100\n",
    "    print(f\"Accuracy achieved : {acc:.2f} on dataset: {len(loader)*batch_size} and mean loss : {sum(val_loss)/len(val_loss):.3f}\")\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
