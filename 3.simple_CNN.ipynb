{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper code from Training session for FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn # all neural network modules, nn.Linear, nn.Conv2d, BatchNorm, loss functions\n",
    "import torch.optim as optim # all optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F # all functions that dont have any parameters eg: activations like relu\n",
    "from torch.utils.data import DataLoader # gives easier dataset management and creates mini batches\n",
    "import torchvision.datasets as datasets # standard public datasets \n",
    "import torchvision.transforms as transforms # transforms on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create Convolutional Neural Network\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, inchannels, num_classes):\n",
    "        super().__init__()\n",
    "         # same convolution where conv input = conv output\n",
    "         # formula for conv output\n",
    "         #  n_out_x = ((in_x + 2*p_x - K_x)/s_x) + 1\n",
    "         # where in_x=input_features in x dim\n",
    "            # p_x=padding x dim, K_x=kernel size x dim, s_x=stride x dim\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=inchannels, out_channels=8,\n",
    "                               kernel_size=(3,3), stride=(1,1), padding=(1,1)) # out=28x28x8\n",
    "        # formula for pool output\n",
    "        # n_pool_x = floor( (in_x - k_x)/s_x) + 1 )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2)) # will half the conv layer size # out=14x14x8\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv1.out_channels, out_channels=16,\n",
    "                               kernel_size=(3,3), stride=(1,1), padding=(1,1)) # out=14x14x16\n",
    "        # need to consider inchannels from previous conv and pooling operation\n",
    "        # since pooling op divided features by half bcoz of kernel=(2,2) and stride=(2,2)\n",
    "        # multiply infeatures of linear layer with out_channels or previous layer and pool channels = 14/2=7\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=self.conv2.out_channels * 7 * 7, out_features=7 * 7)\n",
    "        self.fc2 = nn.Linear(in_features=self.fc1.out_features, out_features=num_classes)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        # need to flatten conv features from NxCxHxW to Nxfeatures before sending to fc block\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # returns logits = (batch_size, num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the CNN architecture on dummy data\n",
    "model = CNN(inchannels=1, num_classes=10)\n",
    "x = torch.rand(64, 1, 28, 28)\n",
    "model(x).shape #torch.Size([64, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Set Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Hyperparameters\n",
    "inchannels = 1\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size=64\n",
    "num_epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Data (Simple MNIST)\n",
    "train_dataset=datasets.MNIST(root='dataset/', train=True, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "test_dataset=datasets.MNIST(root='dataset/', train=False, \n",
    "                             transform=transforms.ToTensor(),\n",
    "                             download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "n_iterations = math.ceil(len(train_dataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Initialize network\n",
    "model = CNN(inchannels==inchannels, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Batch_size/Iterations: 0/938, Train Loss: 2.2985475063323975\n",
      "Epoch : 0 Batch_size/Iterations: 300/938, Train Loss: 0.26051095128059387\n",
      "Epoch : 0 Batch_size/Iterations: 600/938, Train Loss: 0.11150657385587692\n",
      "Epoch : 0 Batch_size/Iterations: 900/938, Train Loss: 0.1138262078166008\n",
      "Epoch : 2 Batch_size/Iterations: 0/938, Train Loss: 0.18936198949813843\n",
      "Epoch : 2 Batch_size/Iterations: 300/938, Train Loss: 0.10541872680187225\n",
      "Epoch : 2 Batch_size/Iterations: 600/938, Train Loss: 0.014640456065535545\n",
      "Epoch : 2 Batch_size/Iterations: 900/938, Train Loss: 0.04166318476200104\n",
      "Epoch : 4 Batch_size/Iterations: 0/938, Train Loss: 0.2044200748205185\n",
      "Epoch : 4 Batch_size/Iterations: 300/938, Train Loss: 0.06313246488571167\n",
      "Epoch : 4 Batch_size/Iterations: 600/938, Train Loss: 0.10339072346687317\n",
      "Epoch : 4 Batch_size/Iterations: 900/938, Train Loss: 0.00802895799279213\n",
      "Epoch : 6 Batch_size/Iterations: 0/938, Train Loss: 0.017373917624354362\n",
      "Epoch : 6 Batch_size/Iterations: 300/938, Train Loss: 0.013334108516573906\n",
      "Epoch : 6 Batch_size/Iterations: 600/938, Train Loss: 0.002267284784466028\n",
      "Epoch : 6 Batch_size/Iterations: 900/938, Train Loss: 0.039411306381225586\n",
      "Epoch : 8 Batch_size/Iterations: 0/938, Train Loss: 0.051658082753419876\n",
      "Epoch : 8 Batch_size/Iterations: 300/938, Train Loss: 0.012982377782464027\n",
      "Epoch : 8 Batch_size/Iterations: 600/938, Train Loss: 0.015472534112632275\n",
      "Epoch : 8 Batch_size/Iterations: 900/938, Train Loss: 0.01581527292728424\n"
     ]
    }
   ],
   "source": [
    "# 8. Train Network\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device) # torch.Size([64, 1, 28, 28]), torch.Size([64])\n",
    "\n",
    "        # clean past gradients collected during backprop\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass - compute predictions\n",
    "        logits = model(data)\n",
    "        train_loss = criterion(logits, targets)\n",
    "        \n",
    "        if epoch % 2 == 0 and batch_idx % 300 == 0:\n",
    "            print(f\"Epoch : {epoch} Batch_size/Iterations: {batch_idx}/{n_iterations}, Train Loss: {train_loss}\")\n",
    "\n",
    "        # Backward Pass - get the gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        # Update our weights\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking accuracy on train data\n",
      "Accuracy achieved : 99.39 on dataset: 60032 and mean loss : 0.020\n",
      "Checking accuracy on test data\n",
      "Accuracy achieved : 98.91 on dataset: 10048 and mean loss : 0.036\n"
     ]
    }
   ],
   "source": [
    "# 9. Check accuracy on training and test to see how good is our model (Eval)\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on train data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "    num_correct=num_samples=0\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "\n",
    "            logits=model(data) # (batch_size, num_classes)\n",
    "            \n",
    "            # we need max index in dim=1 which holds num_classes values\n",
    "            prediction_index = torch.argmax(torch.softmax(logits, dim=1), dim=1) # torch.size([64])\n",
    "            num_correct += (prediction_index == targets).sum()\n",
    "            num_samples += prediction_index.shape[0]\n",
    "\n",
    "            val_loss.append(criterion(logits, targets)) # add losses for each batch\n",
    "    \n",
    "    acc = (num_correct / num_samples) * 100\n",
    "    print(f\"Accuracy achieved : {acc:.2f} on dataset: {len(loader)*batch_size} and mean loss : {sum(val_loss)/len(val_loss):.3f}\")\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
